{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyNSQGW4p1h0cjRk/Kal2G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAdamDev/ETL-pipeline-for-UK-Employment-data/blob/main/ETL_pipeline_for_UK_regional_employment_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mysql-connector-python"
      ],
      "metadata": {
        "id": "ivM2-gcbyXou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOh6XNJ_3fYG"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "import os\n",
        "import socket\n",
        "import sqlite3\n",
        "import requests\n",
        "import pandas as pd\n",
        "import mysql.connector\n",
        "from sqlite3.dbapi2 import OperationalError"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define helper functions\n",
        "\n",
        "def extract_excel_data(url):\n",
        "    try:\n",
        "        # Check the response was successful\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  \n",
        "\n",
        "        # Load each sheet as a separate DataFrame\n",
        "        dfs = pd.read_excel(response.content, sheet_name=None)\n",
        "        return dfs\n",
        "    except (requests.exceptions.RequestException, pd.errors.ParserError) as e:\n",
        "        print(\"Error occurred during data extraction:\", str(e))\n",
        "        return None\n",
        "\n",
        "def delete_meta_data(data_dict):\n",
        "    if 'Information' in data_dict:\n",
        "        del data_dict['Information']\n",
        "    if 'Contents' in data_dict:\n",
        "        del data_dict['Contents']\n",
        "\n",
        "def assign_new_column_names(region, new_names, sheet):\n",
        "    try:\n",
        "        region.rename(columns=new_names, inplace=True)\n",
        "        region.insert(loc=0, column='Region_Name', value=sheet)\n",
        "    except KeyError as e:\n",
        "        print(\"Error occurred during assigning new column names:\", str(e))\n",
        "        print(\"Check that the key values of new_names correspond to the column names\")\n",
        "\n",
        "def drop_redundant_columns(region):\n",
        "    for column in region.columns:\n",
        "        if 'Unnamed' in column:\n",
        "            region.drop(column, axis=1, inplace=True)\n",
        "\n",
        "def drop_redundant_rows(region, redundant_rows):\n",
        "    region.drop(redundant_rows, inplace=True, errors='ignore')\n",
        "\n",
        "def commit_row_changes(region):\n",
        "    region.reset_index(drop=True, inplace=True)\n",
        "\n",
        "def process_non_numerical_values(region):\n",
        "    for column in region.columns:\n",
        "        for index, entry in region[column].items():\n",
        "            if entry == '-':\n",
        "                region[column][index] = 0\n",
        "            elif entry == '*':\n",
        "                region[column][index] = None\n",
        "\n",
        "def update_column_dtypes(region, new_dtypes):\n",
        "    try:\n",
        "        region = region.astype(new_dtypes)\n",
        "    except KeyError as e:\n",
        "        print(\"Error occurred during assigning new column names:\", str(e))\n",
        "        print(\"Check that the key values of new_dtypes correspond to the column names\")\n",
        "\n",
        "def generate_indentifiers(dataframe):\n",
        "    try:\n",
        "        # Create Region, BIG, and Region_BIG IDs\n",
        "        dataframe['Region_ID'] = dataframe['Region_Name'].factorize()[0]\n",
        "        dataframe['BIG_ID'] = dataframe['BIG_Name'].factorize()[0]\n",
        "        dataframe['Region_BIG_ID'] = pd.MultiIndex.from_frame(dataframe[['Region_Name', 'BIG_Name']]).factorize()[0]\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred during identifier generation:\", str(e))\n",
        "\n",
        "def load_into_database(dataframe):\n",
        "    try:\n",
        "        df_Region = dataframe[['Region_Name']]\n",
        "        df_Region = df_Region.groupby('Region_Name').first().reset_index()\n",
        "        df_BIG = dataframe[['BIG_Name']]\n",
        "        df_BIG = df_BIG.groupby('BIG_Name').first().reset_index()\n",
        "        df_Region_BIG = dataframe[['Region_ID', 'BIG_ID']]\n",
        "        df_FT_Employees =  dataframe[['FT_Public', 'FT_Private', 'FT_Pub_Priv']]\n",
        "        df_PT_Employees = dataframe[['PT_Public', 'PT_Private', 'PT_Pub_Priv']]\n",
        "        df_FTPT_Employees = dataframe[['FTPT_Public', 'FTPT_Private', 'FTPT_Pub_Priv']]\n",
        "        df_All_Employees = dataframe[['All_Public', 'All_Private', 'All_Pub_Priv']]\n",
        "\n",
        "        # Execute database loading within a single try block\n",
        "        try:\n",
        "            df_Region.to_sql('Region', conn, if_exists='replace', index='Region_ID')\n",
        "            df_BIG.to_sql('BIG', conn, if_exists='replace', index='BIG_ID')\n",
        "            df_Region_BIG.to_sql('Region_BIG', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "            cursor.execute(\"DROP TABLE IF EXISTS FT_Employees;\")\n",
        "            df_FT_Employees.to_sql('FT_Employees', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "            cursor.execute(\"DROP TABLE IF EXISTS PT_Employees;\")\n",
        "            df_PT_Employees.to_sql('PT_Employees', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "            cursor.execute(\"DROP TABLE IF EXISTS FTPT_Employees;\")\n",
        "            df_FTPT_Employees.to_sql('FTPT_Employees', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "            cursor.execute(\"DROP TABLE IF EXISTS All_Employees;\")\n",
        "            df_All_Employees.to_sql('All_Employees', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "\n",
        "            for row in cursor:\n",
        "                print(row)\n",
        "\n",
        "        except OperationalError as e:\n",
        "            print(\"Error occurred during database loading:\", str(e))\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(\"Error occurred during DataFrame loading preparation:\", str(e))\n",
        "\n",
        "def clean_data(data_dict, cleaned=None):\n",
        "    \n",
        "    # Updated column names\n",
        "    new_column_names = {\n",
        "    'Table 4 - Regional level employment (thousands) by BIG (public/private sector split)': 'BIG_Name',\n",
        "    'Unnamed: 1': 'FT_Public',\n",
        "    'Unnamed: 2': 'FT_Private',\n",
        "    'Unnamed: 3': 'FT_Pub_Priv',\n",
        "    'Unnamed: 4': 'PT_Public',\n",
        "    'Unnamed: 5': 'PT_Private',\n",
        "    'Unnamed: 6': 'PT_Pub_Priv',\n",
        "    'Unnamed: 7': 'FTPT_Public',\n",
        "    'Unnamed: 8': 'FTPT_Private',\n",
        "    'Unnamed: 9': 'FTPT_Pub_Priv',\n",
        "    'Unnamed: 10': 'All_Public',\n",
        "    'Unnamed: 11': 'All_Private',\n",
        "    'Unnamed: 12': 'All_Pub_Priv'\n",
        "    }\n",
        "\n",
        "    # Updated column dtypes\n",
        "    new_column_dtypes = {\n",
        "    'BIG_Name': str,\n",
        "    'FT_Public': float,\n",
        "    'FT_Private': float,\n",
        "    'FT_Pub_Priv': float,\n",
        "    'PT_Public': float,\n",
        "    'PT_Private': float,\n",
        "    'PT_Pub_Priv': float,\n",
        "    'FTPT_Public': float,\n",
        "    'FTPT_Private': float,\n",
        "    'FTPT_Pub_Priv': float,\n",
        "    'All_Public': float,\n",
        "    'All_Private': float,\n",
        "    'All_Pub_Priv': float \n",
        "    }\n",
        "\n",
        "    # Rows to be dropped\n",
        "    redundant_rows = [0, 1, 2, 21, 22, 23, 24, 25, 26, 27, 28]\n",
        "\n",
        "    if not cleaned:\n",
        "      delete_meta_data(data_dict)\n",
        "\n",
        "      regional_dfs = []\n",
        "\n",
        "      for sheet, region in data_dict.items():\n",
        "          assign_new_column_names(region, new_column_names, sheet)\n",
        "          drop_redundant_columns(region)\n",
        "          drop_redundant_rows(region, redundant_rows)\n",
        "          commit_row_changes(region)\n",
        "          process_non_numerical_values(region)\n",
        "          update_column_dtypes(region, new_column_dtypes)\n",
        "          regional_dfs.append(region)\n",
        "\n",
        "      cleaned_regional_data = pd.concat(regional_dfs, ignore_index=True)\n",
        "      generate_indentifiers(cleaned_regional_data)\n",
        "\n",
        "      return cleaned_regional_data\n",
        "\n",
        "def create_table(cursor, table_name, columns, primary_key=None, foreign_key=None, indexes=None):\n",
        "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns}\"\n",
        "\n",
        "    if primary_key:\n",
        "        create_table_query += f\", PRIMARY KEY ({primary_key})\"\n",
        "\n",
        "    if foreign_key:\n",
        "      for foreign_key in foreign_key:\n",
        "        foreign_key_query = f\"FOREIGN KEY ({foreign_key[0]}) REFERENCES {foreign_key[1]} ({foreign_key[0]})\"\n",
        "        create_table_query += f\", {foreign_key_query}\"\n",
        "\n",
        "    create_table_query += \")\"\n",
        "    cursor.execute(create_table_query)\n",
        "\n",
        "    print(create_table_query)\n",
        "\n",
        "    if indexes:\n",
        "        for index in indexes:\n",
        "            index_query = f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_{index} ON {table_name} ({index})\"\n",
        "            cursor.execute(index_query)\n",
        "\n",
        "def create_database(cursor):\n",
        "\n",
        "    region_columns = ', '.join(['Region_ID int NOT NULL', 'Region_Name varchar(255) NOT NULL'])\n",
        "    BIG_columns = ', '.join(['BIG_ID int NOT NULL', 'BIG_Name varchar(255) NOT NULL'])\n",
        "    region_BIG_columns = ', '.join(['Region_BIG_ID int NOT NULL','Region_ID int NOT NULL', 'BIG_ID int NOT NULL'])\n",
        "    PT_employees_columns = ', '.join(['Region_BIG_ID int NOT NULL','PT_Public float','PT_Private float', 'PT_Pub_Priv float'])\n",
        "    FT_employees_columns = ', '.join(['Region_BIG_ID int NOT NULL','FT_Public float','FT_Private float', 'FT_Pub_Priv float'])\n",
        "    FTPT_employees_columns = ', '.join(['Region_BIG_ID int NOT NULL','FTPT_Public float','FTPT_Private float', 'FTPT_Pub_Priv float'])\n",
        "    All_employee_columns = ', '.join(['Region_BIG_ID int NOT NULL','All_Public float','All_Private float', 'All_Pub_Priv float'])\n",
        "\n",
        "    # Create tables\n",
        "    create_table(cursor, 'Region', region_columns, primary_key='Region_ID', foreign_key=None, indexes=['Region_Name'])\n",
        "    create_table(cursor, 'BIG', BIG_columns, primary_key='BIG_ID', foreign_key=None, indexes=['BIG_Name'])\n",
        "    create_table(cursor, 'Region_BIG', region_BIG_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_ID', 'Region'],['BIG_ID', 'BIG']], indexes=None)\n",
        "    create_table(cursor, 'FT_employees', FT_employees_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_BIG_ID', 'Region_BIG']], indexes=None)\n",
        "    create_table(cursor, 'PT_employees', PT_employees_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_BIG_ID', 'Region_BIG']], indexes=None)\n",
        "    create_table(cursor, 'FTPT_employees', FTPT_employees_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_BIG_ID', 'Region_BIG']], indexes=None)\n",
        "    create_table(cursor, 'All_employees', All_employee_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_BIG_ID', 'Region_BIG']], indexes=None)"
      ],
      "metadata": {
        "id": "n1N_JGvM8B-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(cursor):\n",
        "\n",
        "    # Office of National Statistics data sources\n",
        "    employment_data_url = 'https://www.ons.gov.uk/file?uri=/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/datasets/regionbybroadindustrygroupsicbusinessregisterandemploymentsurveybrestable4/2021provisional/table42021p.xlsx'\n",
        "    jobs_data_url = 'https://www.ons.gov.uk/file?uri=/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/datasets/workforcejobsbyindustryjobs02/current/jobs02mar2023.xls'\n",
        "    earnings_data_url = 'https://www.ons.gov.uk/file?uri=/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/regionbyindustry2digitsicashetable5/2022provisional/ashetable52022provisional.zip'\n",
        "\n",
        "    # Load employment data into DataFrame dictionary by region\n",
        "    employment_data_dict = extract_excel_data(employment_data_url)\n",
        "\n",
        "    # Set flag to track cleaning status\n",
        "    cleaned_status = False\n",
        "\n",
        "    # Clean data and flatten into a single DataFrame\n",
        "    cleaned_regional_data = clean_data(employment_data_dict, cleaned=cleaned_status)\n",
        "\n",
        "    # Create SQLite3 database\n",
        "    create_database(cursor)\n",
        "\n",
        "    # Split data into dataframe tables and load into relational tables\n",
        "    load_into_database(cleaned_regional_data)"
      ],
      "metadata": {
        "id": "_iPirOTOYU_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # Open the database connection\n",
        "    conn = sqlite3.connect('regional_UK_employment.db')\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    # Execute ETL process for regional UK employment data\n",
        "    main(cursor)\n",
        "\n",
        "    # Commit changes and close the connection\n",
        "    conn.commit()\n",
        "    conn.close()"
      ],
      "metadata": {
        "id": "DWTWV3dJBmu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establish test connection\n",
        "conn = sqlite3.connect('regional_UK_employment.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Perform test query to return total employment count by broad industry group (BIG)\n",
        "cursor.execute('''\n",
        "    SELECT b.BIG_Name, ROUND(SUM(a.All_Pub_Priv)) AS UK_Employee_Count\n",
        "    FROM All_Employees a\n",
        "    JOIN Region_BIG rb ON a.Region_BIG_ID = rb.Region_BIG_ID\n",
        "    JOIN BIG b ON rb.BIG_ID = b.BIG_ID\n",
        "    GROUP BY b.BIG_Name\n",
        "    ORDER BY UK_Employee_Count DESC;\n",
        "'''\n",
        ")\n",
        "\n",
        "for row in cursor:\n",
        "  print(row)\n",
        "\n",
        "# Commit changes and close the connection\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "WRHWbsaL5o4g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
