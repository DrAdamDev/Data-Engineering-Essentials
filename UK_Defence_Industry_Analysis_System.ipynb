{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnSYXZDEiFbQ4mJxKipfaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAdamDev/ETL-pipeline-for-UK-Employment-data/blob/main/UK_Defence_Industry_Analysis_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mysql-connector-python"
      ],
      "metadata": {
        "id": "ivM2-gcbyXou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "IOh6XNJ_3fYG"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import re\n",
        "import os\n",
        "import zipfile\n",
        "import mysql.connector\n",
        "import logging\n",
        "import socket\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_excel_data(url):\n",
        "  response = requests.get(url)\n",
        "  # Check that the response is successful\n",
        "  response = requests.get(url)\n",
        "  # Make sure the request was successful\n",
        "  response.raise_for_status()\n",
        "  # Load each sheet as a separate DataFrame\n",
        "  dfs = pd.read_excel(response.content, sheet_name=None)\n",
        "  return dfs\n",
        "\n",
        "def display_data(data_dict):\n",
        "  # Access each DataFrame by sheet name\n",
        "  for sheet_name, df in data_dict.items():\n",
        "      # Perform operations on the DataFrame as required\n",
        "      print(f\"Sheet Name: {sheet_name}\")\n",
        "      print(df.head(25))  # Display the first few rows of the DataFrame\n",
        "\n",
        "def unzip_files(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Save the ZIP file locally\n",
        "    with open('archive.zip', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    # Extract the contents of the ZIP file to a folder\n",
        "    extract_folder = 'extracted_files'\n",
        "    with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_folder)\n",
        "\n",
        "    # Get the path of the folder containing the extracted files\n",
        "    folder_path = os.path.abspath(extract_folder)\n",
        "\n",
        "    return folder_path\n",
        "\n",
        "def get_dict_name(file_name):\n",
        "    # Extract the relevant parts of the file name using regular expressions\n",
        "    regex = r\"Table\\s([\\d\\.]+[a-z]+)\\s+(.*)\\d{4}\"\n",
        "    match = re.search(regex, file_name)\n",
        "    if match:\n",
        "        table_number = match.group(1).replace(\".\", \"\")\n",
        "        description = match.group(2).strip().lower()\n",
        "        # Generate the dictionary name by combining the table number and description\n",
        "        dict_name = f\"{description}_data_dict_{table_number}\"\n",
        "        return dict_name\n",
        "    else:\n",
        "        return None      \n",
        "\n",
        "def load_multiple_excel_data(url):\n",
        "    # Unzip the files and get the path to the folder\n",
        "    folder_path = unzip_files(url)\n",
        "\n",
        "    # Load the Excel files in the folder\n",
        "    data_frames = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        dict_name = get_dict_name(filename)\n",
        "        df = pd.read_excel(file_path, sheet_name=None)\n",
        "        data_frames[dict_name] = df\n",
        "\n",
        "    return data_frames\n",
        "\n",
        "def create_table(cursor, table_name, columns, primary_key=None, foreign_key=None, indexes=None):\n",
        "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns}\"\n",
        "\n",
        "    if primary_key:\n",
        "        create_table_query += f\", PRIMARY KEY ({primary_key})\"\n",
        "\n",
        "    if foreign_key:\n",
        "      for foreign_key in foreign_key:\n",
        "        foreign_key_query = f\"FOREIGN KEY ({foreign_key[0]}) REFERENCES {foreign_key[1]} ({foreign_key[0]})\"\n",
        "        create_table_query += f\", {foreign_key_query}\"\n",
        "\n",
        "    create_table_query += \")\"\n",
        "    cursor.execute(create_table_query)\n",
        "\n",
        "    print(create_table_query)\n",
        "\n",
        "    if indexes:\n",
        "        for index in indexes:\n",
        "            index_query = f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_{index} ON {table_name} ({index})\"\n",
        "            cursor.execute(index_query)\n",
        "\n",
        "def create_database():\n",
        "\n",
        "    region_columns = ', '.join(['Region_ID int NOT NULL', 'Region_Name varchar(255) NOT NULL'])\n",
        "    BIG_columns = ', '.join(['BIG_ID int NOT NULL', 'BIG_Name varchar(255) NOT NULL'])\n",
        "    region_BIG_columns = ', '.join(['Region_BIG_ID int NOT NULL','Region_ID int NOT NULL', 'BIG_ID int NOT NULL'])\n",
        "    PT_employees_columns = ', '.join(['Region_BIG_ID int NOT NULL','PT_Public float','PT_Private float', 'PT_Pub_Priv float'])\n",
        "    FT_employees_columns = ', '.join(['Region_BIG_ID int NOT NULL','FT_Public float','FT_Private float', 'FT_Pub_Priv float'])\n",
        "    Total_employees_columns = ', '.join(['Region_BIG_ID int NOT NULL','FTPT_Public float','FTPT_Private float', 'FTPT_Pub_Priv float'])\n",
        "    Total_employment_columns = ', '.join(['Region_BIG_ID int NOT NULL','All_Public float','All_Private float', 'All_Pub_Priv float'])\n",
        "\n",
        "    # Create tables\n",
        "    create_table(cursor, 'Region', region_columns, primary_key='Region_ID', foreign_key=None, indexes=['Region_Name'])\n",
        "    create_table(cursor, 'BIG', BIG_columns, primary_key='BIG_ID', foreign_key=None, indexes=['BIG_Name'])\n",
        "    create_table(cursor, 'Region_BIG', region_BIG_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_ID', 'Region'],['BIG_ID', 'BIG']], indexes=None)\n",
        "    create_table(cursor, 'FT_employees', FT_employees_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_BIG_ID', 'Region_BIG']], indexes=None)\n",
        "    create_table(cursor, 'PT_employees', PT_employees_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_BIG_ID', 'Region_BIG']], indexes=None)\n",
        "    create_table(cursor, 'FTPT_employees', Total_employees_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_BIG_ID', 'Region_BIG']], indexes=None)\n",
        "    create_table(cursor, 'All_employees', Total_employment_columns, primary_key='Region_BIG_ID', foreign_key=[['Region_BIG_ID', 'Region_BIG']], indexes=None)"
      ],
      "metadata": {
        "id": "n1N_JGvM8B-7"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "employment_data_url = 'https://www.ons.gov.uk/file?uri=/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/datasets/regionbybroadindustrygroupsicbusinessregisterandemploymentsurveybrestable4/2021provisional/table42021p.xlsx'\n",
        "jobs_data_url = 'https://www.ons.gov.uk/file?uri=/employmentandlabourmarket/peopleinwork/employmentandemployeetypes/datasets/workforcejobsbyindustryjobs02/current/jobs02mar2023.xls'\n",
        "earnings_data_url = 'https://www.ons.gov.uk/file?uri=/employmentandlabourmarket/peopleinwork/earningsandworkinghours/datasets/regionbyindustry2digitsicashetable5/2022provisional/ashetable52022provisional.zip'\n",
        "\n",
        "employment_data_dict = load_excel_data(employment_data_url)\n",
        "display_data(employment_data_dict)\n",
        "\n",
        "#jobs_data_dict = load_excel_data(jobs_data_url)\n",
        "# display_data(jobs_data_dict) \n",
        "\n",
        "#earnings_data_dict = load_multiple_excel_data(earnings_data_url)\n",
        "# for data_dict in earnings_data_dict.values():\n",
        "# display_data(data_dict)"
      ],
      "metadata": {
        "id": "_iPirOTOYU_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete meta data (info & contents)\n",
        "if 'Information' in employment_data_dict.keys():\n",
        "  del employment_data_dict['Information']\n",
        "if 'Contents' in employment_data_dict.keys():\n",
        "  del employment_data_dict['Contents']\n",
        "\n",
        "# Assign new column names\n",
        "new_column_names = {\n",
        "    'Table 4 - Regional level employment (thousands) by BIG (public/private sector split)': 'BIG_Name',\n",
        "    'Unnamed: 1': 'FT_Public',\n",
        "    'Unnamed: 2': 'FT_Private',\n",
        "    'Unnamed: 3': 'FT_Pub_Priv',\n",
        "    'Unnamed: 4': 'PT_Public',\n",
        "    'Unnamed: 5': 'PT_Private',\n",
        "    'Unnamed: 6': 'PT_Pub_Priv',\n",
        "    'Unnamed: 7': 'FTPT_Public',\n",
        "    'Unnamed: 8': 'FTPT_Private',\n",
        "    'Unnamed: 9': 'FTPT_Pub_Priv',\n",
        "    'Unnamed: 10': 'All_Public',\n",
        "    'Unnamed: 11': 'All_Private',\n",
        "    'Unnamed: 12': 'All_Pub_Priv'\n",
        "}\n",
        "\n",
        "# Assign new column dtypes\n",
        "new_column_dtypes = {\n",
        "    'BIG_Name': str,\n",
        "    'FT_Public': float,\n",
        "    'FT_Private': float,\n",
        "    'FT_Pub_Priv': float,\n",
        "    'PT_Public': float,\n",
        "    'PT_Private': float,\n",
        "    'PT_Pub_Priv': float,\n",
        "    'FTPT_Public': float,\n",
        "    'FTPT_Private': float,\n",
        "    'FTPT_Pub_Priv': float,\n",
        "    'All_Public': float,\n",
        "    'All_Private': float,\n",
        "    'All_Pub_Priv': float \n",
        "}\n",
        "\n",
        "redundant_rows = [0, 1, 2, 21, 22, 23, 24, 25, 26, 27, 28]\n",
        "\n",
        "regional_dfs = []\n",
        "\n",
        "for sheet, region in employment_data_dict.items():\n",
        "\n",
        "  # Update names of columns\n",
        "  region.rename(columns=new_column_names, inplace=True)\n",
        "  region.insert(loc=0, column='Region_Name', value=sheet)\n",
        "\n",
        "  # Drop redundant columns after updating column names\n",
        "  for column in region.columns:\n",
        "    if 'Unnamed' in column:\n",
        "      region.drop(column, axis=1, inplace=True)\n",
        "\n",
        "  # Drop redundant rows\n",
        "  for rows in redundant_rows:\n",
        "    if rows in region.index:\n",
        "      region.drop(rows, inplace=True)\n",
        "\n",
        "  # Commit row changes by resetting index\n",
        "  region.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  for column in region.columns:\n",
        "    for index, entry in region[column].items():\n",
        "      if entry == '-':\n",
        "        region[column][index] = 0\n",
        "      elif entry == '*':\n",
        "        region[column][index] = None\n",
        "\n",
        "  # Update dtypes of columns\n",
        "  region.astype(dtype=new_column_dtypes)\n",
        "\n",
        "  # Prepare list of regional DataFrames for concatenation\n",
        "  regional_dfs.append(region)\n",
        "\n",
        "# Concatenate regional DataFrames\n",
        "regional_data = pd.concat(regional_dfs, ignore_index=True)\n",
        "\n",
        "# Create Region, BIG, and Region_BIG IDs\n",
        "regional_data['Region_ID'] = regional_data['Region_Name'].factorize()[0]\n",
        "regional_data['BIG_ID'] = regional_data['BIG_Name'].factorize()[0]\n",
        "regional_data['Region_BIG_ID'] = pd.MultiIndex.from_frame(regional_data[['Region_Name', 'BIG_Name']]).factorize()[0]"
      ],
      "metadata": {
        "id": "T8z-8A3pK9j4"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('regional_UK_employment.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Create database\n",
        "create_database()\n",
        "\n",
        "# Load data into database\n",
        "df_Region = regional_data[['Region_Name']]\n",
        "df_Region = df_Region.groupby('Region_Name').first().reset_index()\n",
        "df_BIG = regional_data[['BIG_Name']]\n",
        "df_BIG = df_BIG.groupby('BIG_Name').first().reset_index()\n",
        "df_Region_BIG = regional_data[['Region_ID', 'BIG_ID']]\n",
        "df_FT_Employees = regional_data[['FT_Public', 'FT_Private', 'FT_Pub_Priv']]\n",
        "df_PT_Employees = regional_data[['PT_Public', 'PT_Private', 'PT_Pub_Priv']]\n",
        "df_FTPT_Employees = regional_data[['FTPT_Public', 'FTPT_Private', 'FTPT_Pub_Priv']]\n",
        "df_All_Employees = regional_data[['All_Public', 'All_Private', 'All_Pub_Priv']]\n",
        "\n",
        "print(df_Region)\n",
        "print(df_BIG)\n",
        "print(df_Region_BIG)\n",
        "\n",
        "df_Region.to_sql('Region', conn, if_exists='replace', index='Region_ID')\n",
        "df_BIG.to_sql('BIG', conn, if_exists='replace', index='BIG_ID')\n",
        "df_Region_BIG.to_sql('Region_BIG', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "cursor.execute(\"DROP TABLE IF EXISTS FT_Employees;\")\n",
        "df_FT_Employees.to_sql('FT_Employees', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "cursor.execute(\"DROP TABLE IF EXISTS PT_Employees;\")\n",
        "df_PT_Employees.to_sql('PT_Employees', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "cursor.execute(\"DROP TABLE IF EXISTS FTPT_Employees;\")\n",
        "df_FTPT_Employees.to_sql('FTPT_Employees', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "cursor.execute(\"DROP TABLE IF EXISTS All_Employees;\")\n",
        "df_All_Employees.to_sql('All_Employees', conn, if_exists='replace', index='Region_BIG_ID')\n",
        "\n",
        "# Query the database\n",
        "cursor.execute('''\n",
        "    SELECT * FROM sqlite_master WHERE type='table' OR type='column';\n",
        "'''\n",
        ")\n",
        "\n",
        "for row in cursor:\n",
        "  print(row)\n",
        "\n",
        "# Commit changes and close the connection\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "5GcqVkACYFjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the SQLite database\n",
        "conn = sqlite3.connect('regional_UK_employment.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Query the database\n",
        "cursor.execute('''\n",
        "    SELECT * FROM Region;\n",
        "'''\n",
        ")\n",
        "\n",
        "for row in cursor:\n",
        "  print(row)\n",
        "\n",
        "# Commit changes and close the connection\n",
        "conn.commit()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "WRHWbsaL5o4g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}