{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAdamDev/ETL-pipeline-for-UK-Employment-data/blob/main/Rock_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the output directory path\n",
        "output_dir = '/content/'\n",
        "\n",
        "# Get the free space in bytes\n",
        "free_space = os.statvfs(output_dir).f_frsize * os.statvfs(output_dir).f_bavail\n",
        "\n",
        "# Convert to GB\n",
        "free_space_gb = free_space / (1024 ** 3)\n",
        "\n",
        "print(f\"Free space in {output_dir}: {free_space_gb:.2f} GB\")"
      ],
      "metadata": {
        "id": "dpipJYFRFlGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u_naEX1vjB-"
      },
      "outputs": [],
      "source": [
        "# Importing necessary modules for the project\n",
        "import os\n",
        "import cv2\n",
        "import uuid\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.image as image\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "from keras_tuner import HyperModel\n",
        "from sklearn.utils import class_weight\n",
        "from keras_tuner.tuners import Hyperband\n",
        "from keras.optimizers import SGD\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import RMSprop\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras_tuner.engine.hyperparameters import HyperParameters\n",
        "from keras.applications import MobileNetV2, ResNet50\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import BatchNormalization, Concatenate\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ORvXqK7_Sk1"
      },
      "outputs": [],
      "source": [
        "def remove_augmented_images(directories):\n",
        "    for dir in directories:\n",
        "        for root, dirs, files in os.walk(dir):\n",
        "            for file in files:\n",
        "                if '_aug' in file:\n",
        "                    os.remove(os.path.join(root, file))\n",
        "\n",
        "def read_image_data(directories):\n",
        "    i = 0\n",
        "    data = []\n",
        "    problem_images = []\n",
        "    for dir in directories:\n",
        "        for root, dirs, files in os.walk(dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.jpg') | file.endswith('.jpeg'):\n",
        "                    path = os.path.join(root, file)\n",
        "                    try:\n",
        "                        img = cv2.imread(path)\n",
        "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                        data.append((path, os.path.basename(os.path.dirname(path))))\n",
        "                        i += 1\n",
        "                        print(f'Files read: {i}')\n",
        "                    except:\n",
        "                        print(f'Error reading image: {path}')\n",
        "                        problem_images.append(path)\n",
        "                        os.remove(path)\n",
        "    print(f'Problem images: {problem_images}')\n",
        "    return data\n",
        "\n",
        "def augment_data(data):\n",
        "    augmented_data = []\n",
        "    for i, (path, label) in enumerate(data):\n",
        "        print(f'Augmenting image {i+1}/{len(data)}: {path}') # output progress\n",
        "        img = cv2.imread(path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        augmented_data.append((path, label))\n",
        "        for j in range(5):\n",
        "            transform_parameters = {\n",
        "                'theta': np.random.uniform(-40, 40),\n",
        "                'tx': np.random.uniform(-0.2, 0.2),\n",
        "                'ty': np.random.uniform(-0.2, 0.2),\n",
        "                'shear': np.random.uniform(-0.2, 0.2),\n",
        "                'zx': np.random.uniform(0.8, 1.2),\n",
        "                'zy': np.random.uniform(0.8, 1.2),\n",
        "                'flip_horizontal': np.random.random() < 0.5,\n",
        "                'flip_vertical': np.random.random() < 0.5,\n",
        "                'brightness': np.random.uniform(0.5, 1.2),\n",
        "            }\n",
        "\n",
        "            datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "            img_transformed = datagen.apply_transform(img, transform_parameters)\n",
        "            img_path = os.path.splitext(path)[0] + '_aug' + str(j) + '.jpg'\n",
        "            cv2.imwrite(img_path, cv2.cvtColor(img_transformed, cv2.COLOR_RGB2BGR))\n",
        "            augmented_data.append((img_path, label))\n",
        "    return augmented_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44NSIBd9_rMg"
      },
      "outputs": [],
      "source": [
        "def split_data(data):\n",
        "    data_dict = {}\n",
        "    for path, label in data:\n",
        "        key = os.path.splitext(path)[0]\n",
        "        if '_aug' in key:\n",
        "            key = key[:key.rindex('_aug')]\n",
        "        if key not in data_dict:\n",
        "            data_dict[key] = []\n",
        "        data_dict[key].append((path, label))\n",
        "\n",
        "    train_keys, val_keys = train_test_split(list(data_dict.keys()), test_size=0.1, random_state=42)\n",
        "\n",
        "    train_data_dict = {key: data_dict[key] for key in train_keys}\n",
        "\n",
        "    val_data_dict = {key: data_dict[key] for key in val_keys}\n",
        "\n",
        "    train_data = [(path, label) for sublist in train_data_dict.values() for path, label in sublist]\n",
        "    val_data = [(path, label) for sublist in val_data_dict.values() for path, label in sublist]\n",
        "\n",
        "    # Shuffle the training and validation sets\n",
        "    random.shuffle(train_data)\n",
        "    random.shuffle(val_data)\n",
        "\n",
        "    train_df = pd.DataFrame(train_data, columns=['path', 'label'])\n",
        "    val_df = pd.DataFrame(val_data, columns=['path', 'label'])\n",
        "\n",
        "    return train_df, val_df\n",
        "\n",
        "def create_generators(train_df, val_df, image_size, batch_size):\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input,\n",
        "    )\n",
        "\n",
        "    val_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=preprocess_input,\n",
        "    )\n",
        "\n",
        "    train_generator = train_datagen.flow_from_dataframe(\n",
        "        train_df,\n",
        "        x_col='path',\n",
        "        y_col='label',\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        classes=['Basalt', 'Granite', 'Marble', 'Quartzite', 'Coal', 'Limestone', 'Sandstone']\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow_from_dataframe(\n",
        "        val_df,\n",
        "        x_col='path',\n",
        "        y_col='label',\n",
        "        target_size=(image_size, image_size),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        classes=['Basalt', 'Granite', 'Marble', 'Quartzite', 'Coal', 'Limestone', 'Sandstone']\n",
        "    )\n",
        "\n",
        "    return train_generator, val_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1bmz0flAb4l"
      },
      "outputs": [],
      "source": [
        "def calculate_class_weights(train_generator):\n",
        "    class_weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced', \n",
        "        classes=np.unique(train_generator.classes), \n",
        "        y=train_generator.classes\n",
        "    )\n",
        "\n",
        "    class_weights_dict = dict(enumerate(class_weights))\n",
        "    return class_weights_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdOl3hp38aBJ"
      },
      "outputs": [],
      "source": [
        "# Hypermodel class for input into Hyperband tuner\n",
        "class MyHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape, num_classes):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "    def build(self, hp):\n",
        "        # Define hyperparameters\n",
        "        learning_rate = hp.Choice('learning_rate', values=[1e-4, 1e-5])\n",
        "        optimizer = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
        "        base_model_name = hp.Choice('base_model_name', values=['MobileNetV2', 'ResNet50'])\n",
        "        num_conv_blocks_to_freeze = hp.Int('num_conv_blocks_to_freeze', min_value=1, max_value=5, step=1)\n",
        "        num_fc_layers = hp.Int('num_fc_layers', min_value=1, max_value=3, step=1)\n",
        "        num_fc_neurons = hp.Choice('num_fc_neurons', values=[64, 128, 256])\n",
        "        dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        batch_size = hp.Choice('batch_size', values=[32, 64, 128])\n",
        "        num_epochs = hp.Choice('num_epochs', values=[10, 20, 30, 40, 50])\n",
        "\n",
        "        # Define base model\n",
        "        if base_model_name == 'MobileNetV2':\n",
        "            base_model = MobileNetV2(input_shape=self.input_shape, include_top=False, weights='imagenet')\n",
        "        else:\n",
        "            base_model = ResNet50(input_shape=self.input_shape, include_top=False, weights='imagenet')\n",
        "            \n",
        "        # Freeze some convolutional blocks\n",
        "        for i, layer in enumerate(base_model.layers):\n",
        "            if i < len(base_model.layers) - num_conv_blocks_to_freeze:\n",
        "                layer.trainable = False\n",
        "        \n",
        "        # Add fully connected layers\n",
        "        x = base_model.output\n",
        "        x = Flatten()(x)\n",
        "        for i in range(num_fc_layers):\n",
        "            x = Dense(num_fc_neurons, activation='relu')(x)\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "        predictions = Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        # Build and compile the model\n",
        "        if optimizer == 'adam':\n",
        "            optimizer = Adam(learning_rate=learning_rate)\n",
        "        elif optimizer == 'sgd':\n",
        "            optimizer = SGD(learning_rate=learning_rate)\n",
        "        else:\n",
        "            optimizer = RMSprop(learning_rate=learning_rate)\n",
        "            \n",
        "        model = Model(inputs=base_model.input, outputs=predictions)\n",
        "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raLKCTQ_RDoy"
      },
      "outputs": [],
      "source": [
        "# Function for building the model manually with custom hyperparameters\n",
        "def build_model(input_shape, num_classes, learning_rate, optimizer, base_model_name, num_conv_blocks_to_freeze, num_fc_layers, num_fc_neurons, dropout_rate):\n",
        "    \n",
        "    # Define base model\n",
        "    if base_model_name == 'MobileNetV2':\n",
        "        base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "    else:\n",
        "        base_model = ResNet50(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "            \n",
        "    # Freeze some convolutional blocks\n",
        "    for i, layer in enumerate(base_model.layers):\n",
        "        if i < len(base_model.layers) - num_conv_blocks_to_freeze:\n",
        "            layer.trainable = False\n",
        "        \n",
        "    # Add fully connected layers\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    for i in range(num_fc_layers):\n",
        "        x = Dense(num_fc_neurons, activation='relu')(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Build and compile the model\n",
        "    if optimizer == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        optimizer = RMSprop(learning_rate=learning_rate)\n",
        "            \n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J6JZq9ZcT7r"
      },
      "outputs": [],
      "source": [
        "def define_early_stop(monitor, patience):\n",
        "    #Define early stopping criteria\n",
        "    return EarlyStopping(monitor=monitor, patience=patience)\n",
        "\n",
        "def set_steps_per_epoch(train_generator):\n",
        "    #Calculate the number of steps per epoch\n",
        "    return train_generator.n // train_generator.batch_size\n",
        "\n",
        "def set_hyperband_search(input_shape, num_classes):\n",
        "    \"\"\"Set up the hyperparameter search.\"\"\"\n",
        "    hypermodel = MyHyperModel(input_shape=input_shape, num_classes=num_classes)\n",
        "    return Hyperband(hypermodel,\n",
        "                     max_epochs=50,\n",
        "                     factor=3,\n",
        "                     hyperband_iterations=2,\n",
        "                     objective='val_accuracy',\n",
        "                     directory='my_dir',\n",
        "                     project_name='my_project',\n",
        "                     seed=36)\n",
        "    \n",
        "def search_for_hyperparameters(tuner, train_generator, val_generator, steps_per_epoch_train, class_weights_dict):\n",
        "    \"\"\"Search for the optimal hyperparameters.\"\"\"\n",
        "    tuner.search(train_generator,\n",
        "                 steps_per_epoch=steps_per_epoch_train,\n",
        "                 validation_data=val_generator,\n",
        "                 epochs=50,\n",
        "                 callbacks=[define_early_stop()],\n",
        "                 class_weight=class_weights_dict,\n",
        "                 verbose=1)\n",
        "\n",
        "\n",
        "def print_optimal_hyperparameters(tuner):\n",
        "    \"\"\"Print the optimal hyperparameters.\"\"\"\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    print(f'Learning rate: {best_hps.get(\"learning_rate\")}')\n",
        "    print(f'Base model name: {best_hps.get(\"base_model_name\")}')\n",
        "    print(f'Number of convolutional blocks to freeze: {best_hps.get(\"num_conv_blocks_to_freeze\")}')\n",
        "    print(f'Number of fully connected layers: {best_hps.get(\"num_fc_layers\")}')\n",
        "    print(f'Number of neurons in the fully connected layers: {best_hps.get(\"num_fc_neurons\")}')\n",
        "    print(f'Dropout rate: {best_hps.get(\"dropout_rate\")}')\n",
        "    print(f'Batch size: {best_hps.get(\"batch_size\")}')\n",
        "    print(f'Number of epochs: {best_hps.get(\"num_epochs\")}')\n",
        "\n",
        "def get_best_model(tuner):\n",
        "    \"\"\"Get the best model from the search.\"\"\"\n",
        "    return tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iwxkf3w-kvpv"
      },
      "outputs": [],
      "source": [
        "def train_best_model(best_model, train_generator, val_generator, early_stop, steps_per_epoch_train, class_weights_dict):\n",
        "        history = best_model.fit(\n",
        "        train_generator,\n",
        "        epochs=9,\n",
        "        validation_data=val_generator,\n",
        "        steps_per_epoch=steps_per_epoch_train,\n",
        "        callbacks=[early_stop],\n",
        "        class_weight=class_weights_dict\n",
        "    )\n",
        "\n",
        "        return history\n",
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "   #Plots the loss and accuracy over time.\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hohfBhchxCW_"
      },
      "outputs": [],
      "source": [
        "# Main script\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "    # Set paths for data\n",
        "    directories = ['/content/drive/MyDrive/RockData/Dataset/Igneous',\n",
        "                   '/content/drive/MyDrive/RockData/Dataset/Metamorphic',\n",
        "                   '/content/drive/MyDrive/RockData/Dataset/Sedimentary']\n",
        "\n",
        "    # Set image size and batch size\n",
        "    image_size = 224\n",
        "    batch_size = 64\n",
        "\n",
        "    # # Delete all augmented images in the directories\n",
        "    #remove_augmented_images(directories)\n",
        "\n",
        "    # # Perform data augmentation and append the resulting images to the original data\n",
        "    #augment_data(data)\n",
        "\n",
        "    # Create dataframe to hold image paths and labels\n",
        "    data = read_image_data(directories)    \n",
        "\n",
        "    # Split the data into train and validation sets\n",
        "    train_df, val_df = split_data(data)\n",
        "\n",
        "    # Set up the generators with the modified data\n",
        "    train_generator, val_generator = create_generators(train_df, val_df, image_size, batch_size)\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_weights_dict = calculate_class_weights(train_generator)\n",
        "\n",
        "    # Add the remaining code for model training, evaluation, etc.\n",
        "\n",
        "    #Define early stopping criteria\n",
        "    early_stop = define_early_stop(monitor='val_loss', patience=5)\n",
        "\n",
        "    #Calculate the number of steps per epoch\n",
        "    steps_per_epoch_train = set_steps_per_epoch(train_generator)\n",
        "\n",
        "    # Set up the hyperparameter search\n",
        "    #tuner = set_hyperband_search(input_shape=(224, 224, 3), num_classes=7)\n",
        "\n",
        "    # Search for the optimal hyperparameters\n",
        "    #search_for_hyperparameters(tuner, train_generator, val_generator, steps_per_epoch_train, class_weights_dict)\n",
        "\n",
        "    # Print the optimal hyperparameters\n",
        "    #print_optimal_hyperparameters(tuner)\n",
        "\n",
        "    # Get the best model from the search\n",
        "    #best_model = get_best_model(tuner)\n",
        "\n",
        "    # Train the best model found during hyperparameter search\n",
        "    #history = train_best_model(best_model, train_generator, val_generator, early_stop, steps_per_epoch_train, class_weights_dict)\n",
        "\n",
        "    # Plot the training history\n",
        "    #plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "knzS8Vj_xJ3p"
      },
      "outputs": [],
      "source": [
        "# Set up the hyperparameter search\n",
        "hypermodel = MyHyperModel(input_shape=(224, 224, 3), num_classes=7)\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    hyperband_iterations=2,\n",
        "    objective='val_accuracy',\n",
        "    directory='my_dir',\n",
        "    project_name='my_project',\n",
        "    seed=36\n",
        ")\n",
        "\n",
        "# Search for the optimal hyperparameters\n",
        "#with strategy.scope():\n",
        "tuner.search(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch_train,\n",
        "    validation_data=val_generator,\n",
        "    epochs=50,\n",
        "    callbacks=[early_stop],\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Print the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f'Learning rate: {best_hps.get(\"learning_rate\")}')\n",
        "print(f'Base model name: {best_hps.get(\"base_model_name\")}')\n",
        "print(f'Number of convolutional blocks to freeze: {best_hps.get(\"num_conv_blocks_to_freeze\")}')\n",
        "print(f'Number of fully connected layers: {best_hps.get(\"num_fc_layers\")}')\n",
        "print(f'Number of neurons in the fully connected layers: {best_hps.get(\"num_fc_neurons\")}')\n",
        "print(f'Dropout rate: {best_hps.get(\"dropout_rate\")}')\n",
        "print(f'Batch size: {best_hps.get(\"batch_size\")}')\n",
        "print(f'Number of epochs: {best_hps.get(\"num_epochs\")}')\n",
        "\n",
        "# Get the best model from the search\n",
        "best_model = tuner.get_best_models(num_models=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_2pE4hwcXxg"
      },
      "outputs": [],
      "source": [
        "# MANUAL MODEL BUILD\n",
        "\n",
        "# Current Best Configuration with leaking\n",
        "learning_rate = 1e-5\n",
        "optimizer = 'adam'\n",
        "base_model_name = 'ResNet50'\n",
        "num_conv_blocks_to_freeze = 4\n",
        "num_fc_layers = 3\n",
        "num_fc_neurons = 256\n",
        "dropout_rate = 0\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "\n",
        "# Current Best Configuration without leaking\n",
        "learning_rate = 1e-4\n",
        "optimizer = 'adam'\n",
        "base_model_name = 'ResNet50'\n",
        "num_conv_blocks_to_freeze = 1\n",
        "num_fc_layers = 1\n",
        "num_fc_neurons = 128\n",
        "dropout_rate = 0.4\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# Build model based on manually selected hyperparameters\n",
        "model = build_model(input_shape=(224, 224, 3),\n",
        "                    num_classes=7,\n",
        "                    learning_rate=learning_rate,\n",
        "                    optimizer=optimizer,\n",
        "                    base_model_name=base_model_name,\n",
        "                    num_conv_blocks_to_freeze=num_conv_blocks_to_freeze,\n",
        "                    num_fc_layers=num_fc_layers,\n",
        "                    num_fc_neurons=num_fc_neurons,\n",
        "                    dropout_rate=dropout_rate)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "          train_generator,\n",
        "          epochs=9,\n",
        "          validation_data=val_generator,\n",
        "          steps_per_epoch=steps_per_epoch_train,\n",
        "          callbacks=[early_stop],\n",
        "          class_weight=class_weights_dict\n",
        ")\n",
        "\n",
        "# Evaluate model on the validation data\n",
        "val_loss, val_acc = model.evaluate(val_generator)\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Validation accuracy: {val_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvOgqVTjuS-y"
      },
      "outputs": [],
      "source": [
        "# CREATE TFLITE MODEL AND SAVE BOTH .h5 and tflite MODELS\n",
        "\n",
        "# Save the trained model\n",
        "#best_model.save('/content/drive/MyDrive/RockData/my_dir/best_model.h5')\n",
        "\n",
        "# Convert the Keras model to a TensorFlow Lite model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model to a file\n",
        "with open('/content/drive/MyDrive/RockData/my_dir/best_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4N3_pBdAVE_"
      },
      "outputs": [],
      "source": [
        "# Evaluate the saved .h5 model on the validation data\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/RockData/my_dir/my_model.h5')\n",
        "\n",
        "# Evaluate the saved model on the validation data\n",
        "val_loss, val_acc = loaded_model.evaluate(val_generator)\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Validation accuracy: {val_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WonW0zgfuci"
      },
      "outputs": [],
      "source": [
        "# Evaluate the saved tflite model on the validation data\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/RockData/my_dir/best_model.tflite')\n",
        "\n",
        "# Evaluate the saved model on the validation data\n",
        "val_loss, val_acc = loaded_model.evaluate(val_generator)\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Validation accuracy: {val_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8yCm91Jyo_O"
      },
      "outputs": [],
      "source": [
        "# TEST THE .h5 MODEL\n",
        "\n",
        "# Set path to image\n",
        "img_path = '/content/drive/MyDrive/RockData/my_dir/new_images/sandstone_sample_1.jpg'\n",
        "\n",
        "# Read in image\n",
        "img = cv2.imread(img_path)\n",
        "\n",
        "# Convert BGR to RGB color space\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Resize image to target size\n",
        "target_size = (224, 224)\n",
        "img = cv2.resize(img, target_size)\n",
        "\n",
        "# Preprocess image\n",
        "img = preprocess_input(img)\n",
        "\n",
        "# Add batch dimension\n",
        "img = np.expand_dims(img, axis=0)\n",
        "\n",
        "# Get prediction on the image\n",
        "prediction = loaded_model.predict(img)\n",
        "\n",
        "# Print the predicted class and probability\n",
        "predicted_class = np.argmax(prediction)\n",
        "class_names = ['Basalt', 'Granite', 'Marble', 'Quartzite', 'Coal', 'Limestone', 'Sandstone']\n",
        "class_prob = prediction[0][predicted_class]\n",
        "print(f'Predicted class: {class_names[predicted_class]}, probability: {class_prob}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4Z81_V-h4Cs"
      },
      "outputs": [],
      "source": [
        "# TEST THE TFLITE MODEL\n",
        "\n",
        "# Load the TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/drive/MyDrive/RockData/my_dir/model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Load and preprocess the image\n",
        "image = cv2.imread(\"/content/drive/MyDrive/RockData/my_dir/new_images/sandstone_sample_1.jpg\")\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image = cv2.resize(image, (224, 224))\n",
        "image = np.expand_dims(image, axis=0)\n",
        "image = preprocess_input(image)\n",
        "\n",
        "# Make a prediction on the image\n",
        "interpreter.set_tensor(input_details[0][\"index\"], image)\n",
        "interpreter.invoke()\n",
        "output_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
        "\n",
        "# Print the predicted class and probability\n",
        "predicted_class = np.argmax(output_data)\n",
        "class_names = ['Basalt', 'Granite', 'Marble', 'Quartzite', 'Coal', 'Limestone', 'Sandstone']\n",
        "class_prob = output_data[0][predicted_class]\n",
        "print(f\"Predicted class: {class_names[predicted_class]}, probability: {class_prob}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium",
      "authorship_tag": "ABX9TyN1KOhazZDUE0+DXuDgpca5",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}